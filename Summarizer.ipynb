{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Summarization has two types:\n",
    "## 1. Extractive Summarization\n",
    "##      -> This type of summarization involves selecting the most important sentences from the original text and then extract them and combine them to make a summary.\n",
    "##      -> It is like doing a copy-paste from the original text to make a summary.\n",
    "##      -> It is like selecting the most important sentences from the original text and then extract them and combine them to make a summary.\n",
    "## Extractive SUmmarization algorithms:\n",
    "##      -> TextRank\n",
    "##      -> LexRank\n",
    "##      -> LSA (Latent Semantic Analysis)\n",
    "##      -> Luhn’s Algorithm\n",
    "## 2. Abstractive Summarization\n",
    "##      -> This type of summarization involves generating entirely new sentences to capture the meaning of the original text.\n",
    "##      -> It is like writing a summary from scratch.\n",
    "##      -> It is like generating entirely new sentences to capture the meaning of the original text.\n",
    "## Abstractive Summarization algorithms:\n",
    "##      -> Since, we are generating from scratch some of the most effective algorithms are based on transformers architecture\n",
    "##      -> GPT, BERT, T5, PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import bs4 as BeautifulSoup\n",
    "import urllib.request  \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def normalize_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]','',sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    sentence = ' '.join(filtered_tokens)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "# fetching the content from the URL\n",
    "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/20th_century')\n",
    "\n",
    "article_read = fetched_data.read()\n",
    "\n",
    "# parsing the URL content and storing in a variable\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
    "\n",
    "# returning <p> tags\n",
    "paragraphs = article_parsed.find_all('p')\n",
    "\n",
    "# looping through the paragraphs and adding them to the variable\n",
    "article_content = ''.join(paragraph.text for paragraph in paragraphs)\n",
    "\n",
    "sentences = sent_tokenize(article_content)\n",
    "# sentences = [normalize_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Extractive Summariation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7621\n"
     ]
    }
   ],
   "source": [
    "## This is an example of extractive summarization\n",
    "## Here, we are using the frequency of words to generate the summary. We assume that the sentences with the most frequent words are the most important sentences.\n",
    "## This is a very basic example of extractive summarization.\n",
    "\n",
    "## Create Word Frequency dictionary\n",
    "words = word_tokenize(article_content)\n",
    "ps = PorterStemmer()\n",
    "freqTable = dict()\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    word = ps.stem(word)\n",
    "    if word not in stop_words:\n",
    "        freqTable[word] = freqTable.get(word, 0) + 1\n",
    "\n",
    "## Calculating Sentence Scores\n",
    "sentence_scores = dict()\n",
    "for sentence in sentences:\n",
    "    key = sentence[:15]\n",
    "    words = word_tokenize(sentence)\n",
    "    non_stop_words = 0\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word in freqTable:\n",
    "            non_stop_words += 1\n",
    "            sentence_scores[key] = sentence_scores.get(key, 0) + freqTable[word]\n",
    "    sentence_scores[key] = sentence_scores.get(key, 0) / non_stop_words\n",
    "\n",
    "## Calculating the threshold\n",
    "threshold = sum(sentence_scores.values()) / len(sentence_scores)\n",
    "\n",
    "## Generating the summary\n",
    "summary = ''\n",
    "for sentence in sentences:\n",
    "    if sentence_scores[sentence[:15]] > threshold:\n",
    "        summary += \" \" + sentence\n",
    "\n",
    "print(len(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exaplantion of textrank algorithm:\n",
    "## 1. Create a graph where the sentences are the vertices and the edges between the sentences are the weights.\n",
    "## 2. The weights are calculated based on the similarity between the sentences.\n",
    "## 3. The similarity between the sentences is calculated using cosine similarity.\n",
    "## 4. The sentences are then ranked based on the weights of the edges.\n",
    "## 5. The top N sentences are then selected to form the summary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 834)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Word Tokenization\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 102)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculating the similarity matrix\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 15520, Summary Length: 784, compression ratio: 0.05\n",
      "[4]\n",
      "The 20th century was dominated by significant geopolitical events that reshaped the political and social structure of the globe: World War I, the Spanish flu pandemic, World War II and the Cold War. The world was undergoing its second major period of globalization; the first, which started in the 18th century, having been terminated by World War I. World population increased from about 1.6 billion people in 1901 to 6.1 billion at the century's end. The people of the Indian subcontinent, a sixth of the world population at the end of the 20th century, had attained an indigenous independence for the first time in centuries. At the beginning of the period, the British Empire was the world's most powerful nation,[7] having acted as the world's policeman for the past century.\n"
     ]
    }
   ],
   "source": [
    "## Calculating textRank Scores\n",
    "## The TextRank algorithm doesn't just select the sentences that are most similar to other sentences. \n",
    "## It selects the sentences that are most \"important\", \n",
    "## and importance is determined by a sentence's ability to represent other sentences in the document.\n",
    "\n",
    "## In the TextRank algorithm, a sentence is considered important if it is similar to many other sentences that are themselves important. \n",
    "## This is similar to how web pages are ranked by Google's PageRank algorithm, which inspired TextRank.\n",
    "\n",
    "## So, a sentence will have a high rank if it is similar to other sentences that also have high ranks. \n",
    "## This creates a kind of recursive definition of importance.\n",
    "\n",
    "\n",
    "textrank_scores = np.ones(len(sentences))\n",
    "\n",
    "## the damping factor represents the likelihood that the algorithm will randomly jump from one sentence to another during the calculation of the TextRank scores\n",
    "damping_factor = 0.85\n",
    "\n",
    "## TextRank algorithm\n",
    "for iter in range(10):\n",
    "    new_textrank_scores = np.ones(len(sentences)) * (1 - damping_factor)\n",
    "\n",
    "    for sent_1_idx in range(len(sentences)):\n",
    "        for sent_2_idx in range(len(sentences)):\n",
    "            if sent_1_idx != sent_2_idx:\n",
    "                new_textrank_scores[sent_1_idx] += (damping_factor * similarity_matrix[sent_2_idx][sent_1_idx] * textrank_scores[sent_2_idx])\n",
    "    \n",
    "    textrank_scores = new_textrank_scores\n",
    "\n",
    "\n",
    "## Generating the summary\n",
    "ranked_sentences = [sentence for _, sentence in sorted(zip(textrank_scores, sentences), reverse=True)]\n",
    "summary = ' '.join(ranked_sentences[:5])\n",
    "print(f\"Original Length: {len(article_content)}, Summary Length: {len(summary)}, compression ratio: {len(summary)/len(article_content):.2f}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSA (Latent Semantic Analysis) analyzes relationships between a set of documents and the terms they contain.\n",
    "## LSA is based on the principle that words that are close in meaning will occur in similar pieces of text.\n",
    "## LSA is used to find the relationships between the words and the sentences in the text and then rank the sentences based on these relationships.\n",
    "## These relationships are found using singular value decomposition (SVD) which is a matrix factorization technique.\n",
    "\n",
    "## Steps:\n",
    "## 1. Create a term-document matrix where the rows are the terms and the columns are the documents.\n",
    "## 2. Apply SVD to the term-document matrix to get the three matrices U, S, and V.\n",
    "## 3. The matrix U represents the relationship between the terms and the concepts.\n",
    "## 4. The matrix S represents the strength of each concept.\n",
    "## 5. The matrix V represents the relationship between the concepts and the documents.\n",
    "## 6. The sentences are then ranked based on the relationship between the terms and the concepts.\n",
    "## 7. The top N sentences are then selected to form the summary.\n",
    "\n",
    "## X = USV^T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 934) (934, 102)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "document_term_matrix = vectorizer.fit_transform(sentences)\n",
    "document_term_matrix = document_term_matrix.toarray()\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "term_document_matrix = document_term_matrix.T\n",
    "print(document_term_matrix.shape, term_document_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 15520, Summary Length: 824, compression ratio: 0.05\n",
      "After the victory of the Allies in Europe, the war in Asia ended with the Soviet invasion of Manchuria and the dropping of two atomic bombs on Japan by the US, the first nation to develop nuclear weapons and the only one to use them in warfare. The people of the Indian subcontinent, a sixth of the world population at the end of the 20th century, had attained an indigenous independence for the first time in centuries. At the beginning of the period, the British Empire was the world's most powerful nation,[7] having acted as the world's policeman for the past century. Later in the 20th century, the development of computers led to the establishment of a theory of computation. In the latter half of the century, most of the European-colonized world in Africa and Asia gained independence in a process of decolonization.\n"
     ]
    }
   ],
   "source": [
    "## Perform LSA - text summarization\n",
    "\n",
    "## Sklearn implementation for LSA\n",
    "lsa = TruncatedSVD(n_components=1, n_iter=10)\n",
    "lsa.fit(term_document_matrix)\n",
    "\n",
    "## Getting the sentence scores\n",
    "lsa_scores = lsa.components_\n",
    "lsa_scores = lsa_scores / lsa_scores.max()  \n",
    "lsa_scores = lsa_scores * 100\n",
    "\n",
    "## Generating the summary\n",
    "ranked_sentences = [sentence for _, sentence in sorted(zip(lsa_scores[0], sentences), reverse=True)]\n",
    "summary = ' '.join(ranked_sentences[:5])\n",
    "summary = summary.replace('\\n', '')\n",
    "print(f\"Original Length: {len(article_content)}, Summary Length: {len(summary)}, compression ratio: {len(summary)/len(article_content):.2f}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Concept 1 ---\n",
      "Original Length: 15520, Summary Length: 824, compression ratio: 0.05\n",
      "After the victory of the Allies in Europe, the war in Asia ended with the Soviet invasion of Manchuria and the dropping of two atomic bombs on Japan by the US, the first nation to develop nuclear weapons and the only one to use them in warfare. The people of the Indian subcontinent, a sixth of the world population at the end of the 20th century, had attained an indigenous independence for the first time in centuries. At the beginning of the period, the British Empire was the world's most powerful nation,[7] having acted as the world's policeman for the past century. Later in the 20th century, the development of computers led to the establishment of a theory of computation. In the latter half of the century, most of the European-colonized world in Africa and Asia gained independence in a process of decolonization.\n",
      "\n",
      "--- Concept 2 ---\n",
      "Original Length: 15520, Summary Length: 875, compression ratio: 0.06\n",
      "After more than four years of trench warfare in Western Europe, and up to 22 million dead, the powers that had formed the Triple Entente (France, Britain, and Russia, later replaced by the United States and joined by Italy and Romania) emerged victorious over the Central Powers (Germany, Austria-Hungary, the Ottoman Empire and Bulgaria). After the war, Germany was occupied and divided between the Western powers and the Soviet Union. After some years of dramatic military success, Germany was defeated in 1945, having been invaded by the Soviet Union and Poland from the East and by the United States, the United Kingdom, Canada, and France from the West. \"[20] It is estimated that approximately 70 million Europeans died through war, violence and famine between 1914 and 1945. East Germany and the rest of Eastern Europe became Soviet puppet states under communist rule.\n",
      "\n",
      "--- Concept 3 ---\n",
      "Original Length: 15520, Summary Length: 661, compression ratio: 0.04\n",
      "Later in the 20th century, the development of computers led to the establishment of a theory of computation. Many new genres of music were established during the 20th century. [44]One of the prominent traits of the 20th century was the dramatic growth of technology. [1][2]  It was the 10th and last century of the 2nd millennium and was marked by new models of scientific understanding, unprecedented scopes of warfare, new modes of communication that would operate at nearly instant speeds, and new forms of art and entertainment. Major themes of the century include decolonization, nationalism, globalization and new forms of intergovernmental organizations.\n",
      "\n",
      "--- Concept 4 ---\n",
      "Original Length: 15520, Summary Length: 716, compression ratio: 0.05\n",
      "World population increased from about 1.6 billion people in 1901 to 6.1 billion at the century's end. The people of the Indian subcontinent, a sixth of the world population at the end of the 20th century, had attained an indigenous independence for the first time in centuries. At the beginning of the period, the British Empire was the world's most powerful nation,[7] having acted as the world's policeman for the past century. Population growth was also unprecedented,[3] as the century started with around 1.6 billion people, and ended with around 6.2 billion. The world was undergoing its second major period of globalization; the first, which started in the 18th century, having been terminated by World War I.\n",
      "\n",
      "--- Concept 5 ---\n",
      "Original Length: 15520, Summary Length: 779, compression ratio: 0.05\n",
      "Population growth was also unprecedented,[3] as the century started with around 1.6 billion people, and ended with around 6.2 billion. With the end of colonialism and the Cold War, nearly a billion people in Africa were left in new nation states. World population increased from about 1.6 billion people in 1901 to 6.1 billion at the century's end. Many new genres of music were established during the 20th century. The US's global military presence spread American culture around the world with the advent of the Hollywood motion picture industry and Broadway, jazz, rock music, and pop music, fast food and hippy counterculture, hip-hop, house music, and disco, as well as street style, all of which came to be identified with the concepts of popular culture and youth culture.\n"
     ]
    }
   ],
   "source": [
    "## LSA for multiple components --> multiple components meaning multiple concepts/topics\n",
    "\n",
    "\n",
    "lsa = TruncatedSVD(n_components=5, n_iter=10)\n",
    "lsa.fit(term_document_matrix)\n",
    "\n",
    "lsa_scores = lsa.components_\n",
    "lsa_scores = lsa_scores / lsa_scores.max(axis=1)[:, np.newaxis]  ## Normalizing the scores for each concept\n",
    "lsa_scores = lsa_scores * 100\n",
    "\n",
    "for component in range(5):\n",
    "    ## Generating the summary\n",
    "    ranked_sentences = [sentence for _, sentence in sorted(zip(lsa_scores[component], sentences), reverse=True)]\n",
    "    summary = ' '.join(ranked_sentences[:5])\n",
    "    summary = summary.replace('\\n', '')\n",
    "\n",
    "    print(f\"\\n--- Concept {component + 1} ---\")\n",
    "    print(f\"Original Length: {len(article_content)}, Summary Length: {len(summary)}, compression ratio: {len(summary)/len(article_content):.2f}\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 15520, Summary Length: 769, compression ratio: 0.05\n",
      "Due to continuing industrialization and expanding trade, many significant changes of the century were, directly or indirectly, economic and technological in nature. World population increased from about 1.6 billion people in 1901 to 6.1 billion at the century's end. The deaths from acts of war during the two world wars alone have been estimated at between 50 and 80 million. By the end of the 20th century, in many parts of the world, women had the same legal rights as men, and racism had come to be seen as unacceptable, a sentiment often backed up by legislation. [19] According to Charles Tilly, \"Altogether, about 100 million people died as a direct result of action by organized military units backed by one government or another over the course of the century.\n"
     ]
    }
   ],
   "source": [
    "## Generate one summary from all the concepts\n",
    "lsa = TruncatedSVD(n_components=20, n_iter=10)\n",
    "lsa.fit(term_document_matrix)\n",
    "\n",
    "lsa_scores = lsa.components_\n",
    "lsa_scores = lsa_scores / lsa_scores.max(axis=1)[:, np.newaxis] # normalize the scores\n",
    "lsa_scores = lsa_scores * 100\n",
    "\n",
    "## Generating the summary\n",
    "ranked_sentences = [sentence for _, sentence in sorted(zip(lsa_scores.sum(axis=0), sentences), reverse=True)]\n",
    "summary = ' '.join(ranked_sentences[:5])\n",
    "summary = summary.replace('\\n', '')\n",
    "print(f\"Original Length: {len(article_content)}, Summary Length: {len(summary)}, compression ratio: {len(summary)/len(article_content):.2f}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
